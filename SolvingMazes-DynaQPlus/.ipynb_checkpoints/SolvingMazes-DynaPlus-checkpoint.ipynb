{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **SolvingMazes: Dyna-Q+ Agent**\n",
    "[Achi's Projects](https://github.com/QuantumNano-AI/PROJECTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The DynaQ+ algorithm is more robust than DynaQ. It provides a solution to environment dynamics changing and thus making its model inaccurate**\n",
    "\n",
    "**For the DynaQ+ Agent we will be adding two new parameters `state visitation count` $\\tau$ and `scaling parameter` $\\kappa$.**\n",
    "\n",
    "**Where $r$ is the modeled reward for a transition which hasn't been visited in $\\tau(s,a)$ time steps, the planning updates are done using $r + \\kappa \\sqrt{\\tau(s,a)}$ as the reward instead**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.preprocessing as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import operator\n",
    "        \n",
    "class Maze:\n",
    "    def __init__(self, maze, rewards = {'goal':1000000,'wall':-15, 'other':-1}):\n",
    "                \n",
    "        \"\"\"# Read file and set height and width of maze\n",
    "        with open(filename) as f:\n",
    "            maze = f.read()\"\"\"\n",
    "        self.rewards=rewards\n",
    "        # Validate start and goal\n",
    "        if maze.count(\"A\") != 1:\n",
    "            raise Exception(\"maze must have exactly one start point\")\n",
    "        if maze.count(\"B\") != 1:\n",
    "            raise Exception(\"maze must have exactly one goal\")\n",
    "        \n",
    "        self.actions =  [\"up\", \"down\", \"left\", \"right\"]\n",
    "        \n",
    "        # Determine height and width of maze\n",
    "        maze = maze.splitlines()\n",
    "        self.height = len(maze)\n",
    "        self.width = max(len(line) for line in maze)\n",
    "        self.states = []\n",
    "        \n",
    "                \n",
    "        # Keep track of walls\n",
    "        self.walls = []\n",
    "        self.wall_cords = []\n",
    "        for i in range(self.height):\n",
    "            row = []\n",
    "            for j in range(self.width):\n",
    "                try:\n",
    "                    if maze[i][j] == \"A\":\n",
    "                        self.start = (i, j)\n",
    "                        row.append(False)\n",
    "                        self.states.append((i,j))\n",
    "                    elif maze[i][j] == \"B\":\n",
    "                        self.goal = (i, j)\n",
    "                        row.append(False)\n",
    "                        self.states.append((i,j))\n",
    "                    elif maze[i][j] == \" \":\n",
    "                        row.append(False)\n",
    "                        self.states.append((i,j))\n",
    "                    else:\n",
    "                        row.append(True)\n",
    "                        self.wall_cords.append((i,j))\n",
    "                except IndexError:\n",
    "                    row.append(False)\n",
    "                    self.states.append((i,j))\n",
    "            self.walls.append(row)\n",
    "            \n",
    "\n",
    "        self.state_count = len(self.states)\n",
    "        self.solution = None\n",
    "        self.V = dict(zip(self.states, self.state_count*[0]))\n",
    "        self.pi = dict(zip(self.states, self.state_count*[0]))\n",
    "        for s in self.states:\n",
    "            avail_actions = self.actions\n",
    "            self.pi[s] = avail_actions[0]\n",
    "        self.pi1 = dict(zip(self.states, self.state_count*[0]))\n",
    "        \n",
    "    def print(self):\n",
    "        solution = self.solution[1] if self.solution is not None else None\n",
    "        print()\n",
    "        for i, row in enumerate(self.walls):\n",
    "            for j, col in enumerate(row):\n",
    "                if col:\n",
    "                    print(\"#\", end=\"\")\n",
    "                elif (i, j) == self.start:\n",
    "                    print(\"A\", end=\"\")\n",
    "                elif (i, j) == self.goal:\n",
    "                    print(\"B\", end=\"\")\n",
    "                elif solution is not None and (i, j) in solution:\n",
    "                    print(\"*\", end=\"\")\n",
    "                else:\n",
    "                    print(\" \", end=\"\")\n",
    "            print()\n",
    "        print()\n",
    "\n",
    "    def neighbors(self, state, a = None):\n",
    "        \"\"\"This function takes in a state and returns all available actions for that state the next state \n",
    "            and reward if each action is take, with a specific transition probability\"\"\"\n",
    "        row, col = state\n",
    "        candidates = [\n",
    "            (\"up\", (row - 1, col)),\n",
    "            (\"down\", (row + 1, col)),\n",
    "            (\"left\", (row, col - 1)),\n",
    "            (\"right\", (row, col + 1))\n",
    "        ]\n",
    "        terminal = False\n",
    "        result = []\n",
    "        for action, (r, c) in candidates:\n",
    "            if (r,c) == self.goal: terminal = True\n",
    "            if 0 <= r < self.height and 0 <= c < self.width and not self.walls[r][c]:\n",
    "                if (row, col) == self.goal: \n",
    "                    (r, c) = self.goal; terminal = True\n",
    "                reward = self.rewards['goal'] if ((r,c) == self.goal) or (state == self.goal) else self.rewards['other']\n",
    "                trans_prob = 1\n",
    "                result.append((action, (r, c), reward, trans_prob, terminal))\n",
    "                \n",
    "        actions = [tup[0] for tup in result]\n",
    "\n",
    "        if a:\n",
    "            R = []\n",
    "            if a in actions:\n",
    "                inx = actions.index(a)\n",
    "                R.append((result[inx]))\n",
    "                return R\n",
    "            else: \n",
    "                R.append((a, (row,col), self.rewards['wall'], 1, terminal))\n",
    "                return R\n",
    "        return result\n",
    "    \n",
    "    def plot_state_values(self):\n",
    "        val = np.array(list(self.V.values())).reshape(-1,1)\n",
    "        va = sc.MinMaxScaler(feature_range=(0, 255)).fit_transform(val).flatten()\n",
    "        V = {}\n",
    "        for i in range(len(va)):\n",
    "            V[list(self.V.keys())[i]] = va[i]\n",
    "\n",
    "        # create a black image\n",
    "        img  = np.ones((self.height,self.width,3), np.uint8)\n",
    "\n",
    "        for item in V.items():\n",
    "            (r,c),vx = item\n",
    "            img[r,c] = [0,vx,0]\n",
    "\n",
    "        for r,c in self.wall_cords:\n",
    "            img[r,c] = [150,5,150]\n",
    "\n",
    "        img[self.start[0],self.start[1]] = [255,0,0]\n",
    "        img[self.goal[0],self.goal[1]] = [100,100,255]\n",
    "        def showimg(img):\n",
    "            plt.figure(figsize = (15,15))\n",
    "            plt.imshow(img, cmap='viridis')\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            #plt.colorbar()\n",
    "            plt.show()\n",
    "        showimg(img)\n",
    "        \n",
    "        def policy_(s):\n",
    "            row, col = s\n",
    "            candidates = [\n",
    "                (\"up\", (row - 1, col)),\n",
    "                (\"down\", (row + 1, col)),\n",
    "                (\"left\", (row, col - 1)),\n",
    "                (\"right\", (row, col + 1))\n",
    "            ]\n",
    "\n",
    "\n",
    "            if s in self.wall_cords:\n",
    "                return ('WALL!!!')\n",
    "            else:\n",
    "                values = {a:self.V[r,c] for a,(r,c) in candidates if 0 <= r < self.height and 0 <= c < self.width and not self.walls[r][c]}\n",
    "                values = {v:k for k,v in values.items()}\n",
    "                best = values[max(values)]\n",
    "            return best\n",
    "        \n",
    "        pi = np.zeros((self.height, self.width)).astype('str')\n",
    "        pi = np.where(pi=='0.0', 'wall', pi)\n",
    "        candidates = {\n",
    "                    \"up\": [255,0,0],\n",
    "                    \"down\": [0,0,255],\n",
    "                    \"left\": [0,255,0],\n",
    "                    \"right\": [255,255,0]\n",
    "        }\n",
    "        for item in self.pi.keys():\n",
    "            action = policy_(item)\n",
    "            \n",
    "            r,c = item\n",
    "            pi[r,c] = action\n",
    "            img[r,c] = candidates[action]\n",
    "            \n",
    "        img[self.start[0],self.start[1]] = [255,0,0]\n",
    "        img[self.goal[0],self.goal[1]] = [100,100,255]\n",
    "        def showimg(img):\n",
    "            plt.figure(figsize = (15,15))\n",
    "            plt.imshow(img, cmap='viridis', )\n",
    "            plt.yticks(list(range(self.height)))\n",
    "            plt.xticks(list(range(self.width)))\n",
    "            plt.title(\"POLICY\\n\\nRED  =>  up\\nBLUE  =>  down\\nGREEN  =>  left\\nYELLOW  =>  right\")\n",
    "            #plt.colorbar()\n",
    "            plt.show()\n",
    "        showimg(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQ(Maze):\n",
    "    def __init__(self, maze, rewards = {'goal':1000000,\n",
    "                                            'wall':-15, \n",
    "                                            'other':-1}, \n",
    "                             info = {} ):\n",
    "        \n",
    "        Maze.__init__(self, maze, rewards)\n",
    "        self.epsilon = info.get('epsilon', 0.9) # Exporation parameter\n",
    "        self.r = np.random.RandomState(seed=12345)\n",
    "        self.episodes =info.get('episodes', 200)\n",
    "        self.max_steps =info.get('max_steps', 1500)\n",
    "        self.alpha = info.get('alpha',0.1) # step size\n",
    "        self.gamma = info.get('gamma',.99) # discount factor\n",
    "        self.kappa = info.get('kappa',.001) # Scalling parameter\n",
    "        self.planning_steps = info.get('planning_steps',50)\n",
    "        self.tau = pd.DataFrame(np.zeros((self.state_count, len(self.actions))),columns=self.actions, index=self.states).to_dict(orient='index')\n",
    "        \n",
    "    def func_q(self, states,n_states,n_actions,kind = 'random'):\n",
    "        if kind=='ones':\n",
    "            return dict(zip(states,np.ones((n_states,n_actions)).tolist()))\n",
    "        elif kind == 'zeros':\n",
    "            return dict(zip(states,np.zeros((n_states,n_actions)).tolist()))\n",
    "        elif kind =='random':\n",
    "            return dict(zip(states,np.round(self.r.randn(n_states,n_actions),2).tolist()))\n",
    "        else :  raise NameError(\"Wrong input: please use ['ones', 'zeros', 'random']\")  \n",
    "    \n",
    "    def argmax(self, test_array):\n",
    "        return self.r.choice(np.flatnonzero(np.array(test_array)==np.array(test_array).max()))\n",
    "    \n",
    "    def epsilon_greedy(self, Q, epsilon, actions, state, train=False):\n",
    "        current_q = Q[state]\n",
    "        if self.r.rand() < epsilon:\n",
    "            action = self.r.choice(actions)\n",
    "            return action\n",
    "        else:\n",
    "            action = self.argmax(current_q)\n",
    "        return actions[action]\n",
    "    \n",
    "    def sigmoid(self,a):\n",
    "        import numpy as np\n",
    "        s = np.divide(1,1+np.exp(-a))\n",
    "        return s\n",
    "    \n",
    "    def run(self):\n",
    "        self.Q = self.func_q(self.states,self.state_count,len(self.actions),kind = 'zeros')\n",
    "        self.model = {} # model is a dictionary of dictionaries, which maps states to actions to (reward, next_state) tuples\n",
    "        def update_model(s,a,s_,reward):\n",
    "            if s in self.model: self.model[s][a] = (s_,reward) # If the agent has been in this state before, update the action/reward\n",
    "            else: \n",
    "                self.model[s] = {a:(s_,reward)} # else add new state and action to model\n",
    "                # actions that had never been tried are now going to be considered. The initial model for such actions would be\n",
    "                # to lead back to the previous state and have a reward of zero\n",
    "                for action in self.actions:\n",
    "                    if action != a:\n",
    "                        self.model[s][a] = (s, 0)\n",
    "                \n",
    "        def planning():\n",
    "            for i in range(self.planning_steps):\n",
    "                s = list(self.model.keys())[self.r.randint(len(self.model.keys()))]\n",
    "                a = self.r.choice(list(self.model[s].keys()))\n",
    "                (s_,reward) = self.model[s][a]\n",
    "                reward += self.kappa*np.sqrt(self.tau[s][a])\n",
    "                q_ = self.Q[s_]\n",
    "                a_ = self.actions[self.argmax(q_)] # Action in the next state does not follow policy. It is rather selected to maximise utility.\n",
    "                if terminal:\n",
    "                    self.Q[s][self.actions.index(a)] += self.alpha * (reward - self.Q[s][self.actions.index(a)])\n",
    "                else:\n",
    "                    self.Q[s][self.actions.index(a)] += self.alpha * (reward + self.gamma*max(q_) \\\n",
    "                                                                      - self.Q[s][self.actions.index(a)])  \n",
    "                \n",
    "        self.time_steps = pd.DataFrame()\n",
    "\n",
    "        for episode in range(self.episodes):\n",
    "\n",
    "            total_reward = 0 # This sets the total reward obtained during this episode\n",
    "            s = self.states[self.r.randint(len(self.states))]\n",
    "            a = self.epsilon_greedy(Q=self.Q, epsilon=self.epsilon, actions=self.actions, state=s)\n",
    "            t = 0 \n",
    "            terminal = False\n",
    "            while t < self.max_steps:\n",
    "                t+=1\n",
    "                self.tau = {k:{kk:vv+1 for kk,vv in v.items()} for k,v in self.tau.items()}\n",
    "                self.tau[s][a] = 0\n",
    "                \n",
    "                _,s_, reward, p,terminal = self.neighbors(s,a)[0]\n",
    "                total_reward += reward\n",
    "                q_ = self.Q[s_] # Action values in the next state\n",
    "                a_ = self.epsilon_greedy(Q=self.Q, epsilon=self.epsilon, actions=self.actions, state=s)\n",
    "                reward += self.kappa*np.sqrt(self.tau[s][a])\n",
    "                if terminal:\n",
    "                    self.Q[s][self.actions.index(a)] += self.alpha * (reward - self.Q[s][self.actions.index(a)])\n",
    "                else:\n",
    "                    self.Q[s][self.actions.index(a)] += self.alpha * (reward + self.gamma*self.Q[s_][self.actions.index(a_)] \\\n",
    "                                                                      - self.Q[s][self.actions.index(a)])  \n",
    "                update_model(s,a,s_,reward)\n",
    "                \n",
    "                # Carry out planning only when there is a complete episode with rewards returned\n",
    "                if len(self.time_steps)>0: \n",
    "                    ts = self.time_steps[self.time_steps.rewards!=0]\n",
    "                    if len(ts)>0:planning()\n",
    "                    \n",
    "                s, a = s_, a_\n",
    "                if terminal:\n",
    "                    self.time_steps = self.time_steps.append(\n",
    "                        pd.Series({'episode':int(episode), 'steps':t, 'rewards':total_reward}), ignore_index=True)\n",
    "                    break\n",
    "            if t%10==0:\n",
    "                print(f'.',end='')\n",
    "        self.pi = {}\n",
    "        self.V = {}\n",
    "        for k,v in self.Q.items():\n",
    "            self.pi[(k)] = self.actions[self.argmax(v)]\n",
    "            self.V[(k)] = max(v)\n",
    "        max_r = max([v for k,v in self.V.items()])\n",
    "        self.V[self.goal] = max_r\n",
    "        img = self.plot_state_values()  \n",
    "        #return V,pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze0 = \"\"\"#####################\n",
    "A                  ##\n",
    "## ############## # #\n",
    "## ####### ###### # #\n",
    "##                # #\n",
    "## ####### ######   #\n",
    "##         ###### # #\n",
    "###################B#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################\n",
      "A                  ##\n",
      "## ############## # #\n",
      "## ####### ###### # #\n",
      "##                # #\n",
      "## ####### ######   #\n",
      "##         ###### # #\n",
      "###################B#\n"
     ]
    }
   ],
   "source": [
    "print(maze0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    }
   ],
   "source": [
    "info = {'episodes': 50,'max_steps': 5000,'alpha': 0.1,'epsilon':.1, 'planning_steps':20, 'kappa':.5}\n",
    "rewards = {'goal':1000,'wall':0, 'other':0}\n",
    "m = DynaQ(maze = maze0, rewards = rewards, info = info)\n",
    "m.run()\n",
    "#t = m.time_steps; t[t.rewards!=0][['episode','steps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze = \"\"\"############################\n",
    "##     #####              ##\n",
    "##B### ##### ##### ### ##  #\n",
    "###                ### ### #\n",
    "# ### ##### ########## ### #\n",
    "# ### #####            ### #\n",
    "#           # ####### #### #\n",
    "# #### #### # ####### #### #\n",
    "# ##    ### #              #\n",
    "#  ### #### # ######## ### #\n",
    "## ##   ###       #### ### #\n",
    "## #### #### #### #### ### #\n",
    "## #### #### #### #### ### #\n",
    "##                         #\n",
    "###A########################\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(maze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {'episodes': 500,'max_steps': 5000,'alpha': 0.1,'epsilon':.1, 'planning_steps':10, 'kappa':.5}\n",
    "rewards = {'goal':10000000,'wall':0, 'other':0}\n",
    "m = DynaQ(maze = maze, rewards = rewards, info = info)\n",
    "m.run()\n",
    "#t = m.time_steps; t[t.rewards!=0][['episode','steps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze1 = \"\"\"#####################\n",
    "##                  #\n",
    "## ### ############ #\n",
    "##              ### #\n",
    "## ### ############B#\n",
    "###### ##############\n",
    "##         ######## #\n",
    "## #### ## ######## #\n",
    "## ####             #\n",
    "## ############ ### #\n",
    "## #      ##### ### #\n",
    "## # ####       ### #\n",
    "## # ############## #\n",
    "##                  #\n",
    "## ################ #\n",
    "##A##################\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(maze1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = {'episodes': 50,'max_steps': 5000,'alpha': 0.1,'epsilon':.1, 'planning_steps':20}\n",
    "rewards = {'goal':100000000,'wall':0, 'other':0}\n",
    "m = DynaQ(maze = maze1, rewards = rewards, info = info)\n",
    "m.run()\n",
    "#t = m.time_steps; t[t.rewards!=0][['episode','steps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
